{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Magic: The Gathering archetypes with LDA: Code\n",
    "\n",
    "This notebook is meant as a supplement for [this article](https://medium.com/@hlynurd/finding-magic-the-gathering-archetypes-with-latent-dirichlet-allocation-729112d324a6). The results were obtained by working with [this data](Modern.htm). \n",
    "You can try this method on data from other formats as well. There is an API on <a href=\"https://mtgdecks.net\" rel=\"follow\">MTG Decks</a> to access the latest 500 tournament decklists from <a href=\"https://mtgdecks.net/decks/csv/Standard\" rel=\"follow\">Standard</a>, <a href=\"https://mtgdecks.net/decks/csv/Modern\" rel=\"follow\">Modern</a>,\n",
    "<a href=\"https://mtgdecks.net/decks/csv/Legacy\" rel=\"follow\">Legacy</a>, <a href=\"https://mtgdecks.net/decks/csv/Vintage\" rel=\"follow\">Vintage</a>, <a href=\"https://mtgdecks.net/decks/csv/Commander\" rel=\"follow\">Commander</a>, <a href=\"https://mtgdecks.net/decks/csv/Pauper\" rel=\"follow\">Pauper</a>, <a href=\"https://mtgdecks.net/decks/csv/Frontier\" rel=\"follow\">Frontier</a>, <a href=\"https://mtgdecks.net/decks/csv/Peasant\" rel=\"follow\">Peasant</a>  or <a href=\"https://mtgdecks.net/decks/csv/Highlander\" rel=\"follow\">Highlander</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The usual first step of machine learning tasks is making sure that the data is in the right form for our algorithms. The raw data is a csv file where each line represents a decklist. Each line contains a main deck and sideboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"4 Celestial Colonnade 1 Celestial Purge 3 Cryptic Command 2 Detention Sphere 1 Disdainful Stroke 1 Dispel 1 Elspeth, Sun's Champion 2 Field of Ruin 4 Flooded Strand 1 Geist of Saint Traft 2 Ghost Quarter 1 Gideon Jura 2 Gideon of the Trials 1 Gideon, Ally of Zendikar 3 Glacial Fortress 1 Grafdigger's Cage 1 Hallowed Fountain 5 Island 1 Jace, Architect of Thought 3 Leyline of Sanctity 3 Mana Leak 2 Negate 4 Path to Exile 3 Plains 2 Rest in Peace 1 Search for Azcanta 4 Serum Visions 3 Snapcaster Mage 1 Sphinx's Revelation 4 Spreading Seas 2 Stony Silence 3 Supreme Verdict 1 Temple of Enlightenment 1 Think Twice 1 Vendilion Clique\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Modern.htm', 'r') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the data into a gensim Dictionary, similarly as in [this tutorial](https://radimrehurek.com/gensim/tut1.html). We split each decklist into individual cards, ignoring the card counts and cards that appear only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re \n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([x.strip() for x in re.split(r\"[\\d]+\", line.replace(\"\\\"\", \"\"))] for line in open('Modern.htm'))\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)  # remove cards that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 700 cards in our modern-card dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n"
     ]
    }
   ],
   "source": [
    "unique_cards = len(dictionary.keys())\n",
    "print(unique_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a gensim Corpus. Instead of having a bag of words (cards) model, we take note how many times each card appears in a deck and \"uncompress\" the decklist description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('Modern.htm'):\n",
    "            decklist = line.replace(\"\\\"\", \"\") # remove start and end tokens            \n",
    "            decklist = re.split(r\"([\\d]+)\", decklist) # split by numbers and card names\n",
    "            decklist = [x.strip() for x in decklist] # remove whitespace\n",
    "            decklist = filter(None, decklist) # remove empty words\n",
    "            cleaned_decklist = [] \n",
    "            for i in range(len(decklist)/2): # remove numbers, add multiplicities of cards\n",
    "                for j in range(int(decklist[i*2])):\n",
    "                    cleaned_decklist.append(decklist[i*2+1])\n",
    "            yield dictionary.doc2bow(cleaned_decklist)\n",
    "corpus_memory_friendly = MyCorpus()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now that the data is ready, we set the number of achetypes to be found. Setting it to 30 gave me good results. Try varying this and see what happens! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archetypes = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are stochastic steps in the training of the model, you might get slightly different results each time. Having the seed set to 1 allows you to recreate my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Latent Dirichlet\" part of the method name comes from the assumption that the latent [priors](https://en.wikipedia.org/wiki/Prior_probability) on the per-archetype card distribution and per-decklist archetype distributions are [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution). This allows us to steer the learning of the model.\n",
    "\n",
    "By incorporating such priors, we can tell the model how we believe the data actually looks like. If we have a large number of archetypes and are confident that each decklist only falls under one archetype, then setting a low alpha indicates that we prefer each decklist to belong to few, dominating archetypes. We can similarly control the archetype-card sparsity with beta. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_prior = [1.0 / archetypes] * archetypes\n",
    "beta_prior = [1.0 / archetypes] * unique_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally train the model. This could take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 30\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus_memory_friendly, id2word=dictionary, num_topics=archetypes, passes=iterations, alpha = alpha_prior, eta = beta_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "A good rule of thumb while doing machine learning work is to do regular sanity checks. Anything from simple output prints to beautiful visualizations will help you understand what's going on. After the training is finished, we can explore the archetypes that it finds. Gensim offers a nice way to see the probability-card pairs in each archetype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_top_cards = 16\n",
    "archetypes_to_inspect = 3\n",
    "for i in range(archetypes_to_inspect):\n",
    "    print((\"Archetype %i \\n %s \\n\") % (i, lda.print_topic(i, topn=number_of_top_cards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is generative, we can generate new decks as well. Here's an example of how to make a metagame altering affinity deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archetype_id = 13\n",
    "archetype_topic = np.array(lda.show_topic(archetype_id, topn=9999))\n",
    "\n",
    "archetype_distribution = np.array(archetype_topic[:,1], dtype=\"float32\")\n",
    "archetype_distribution = archetype_distribution / np.sum(archetype_distribution)\n",
    "\n",
    "archetype_indices = np.zeros(len(archetype_distribution))\n",
    "main_deck = 60\n",
    "sideboard = 15\n",
    "while np.sum(archetype_indices) < main_deck+sideboard:\n",
    "    new_card = np.random.multinomial(1, archetype_distribution)\n",
    "    archetype_indices += new_card\n",
    "    if 5 in archetype_indices:\n",
    "        archetype_indices -= new_card\n",
    "archetype_cards = np.array(archetype_topic[:,0], dtype=\"string\")\n",
    "minimum_cards = 1.0\n",
    "deck_title = \"Affinity for AI\"\n",
    "print(deck_title)\n",
    "for i in range(len(archetype_distribution)):\n",
    "    if archetype_indices[i] >= minimum_cards:        \n",
    "        print('%i %s' % (archetype_indices[i], archetype_cards[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "#print(topic[i, 0])\n",
    "#print(topic[i, 1])\n",
    "#probs = np.array(np.array(lda.show_topic(0, topn=9999))[:,1], dtype=\"float32\")\n",
    "#print(np.sum(probs))\n",
    "for j in range(30):\n",
    "    print(\"Archetype %i \" % (j))\n",
    "    for i in range(32):\n",
    "        topic = np.array(lda.show_topic(j, topn=9999))\n",
    "        print('%.3f %s' % (float(topic[i, 1]), topic[i, 0]))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
