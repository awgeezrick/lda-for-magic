{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Magic: The Gathering archetypes with LDA: Code\n",
    "\n",
    "This notebook is meant as a supplement for [this article](https://medium.com/p/729112d324a6). The results were obtained by working with [this data](https:linktothesamedata). You can try this method on data from other decklists. There is an API on [MTGDecks](https://mtgdecks.net) to access the latest 500 tournament decklists from [Standard](https://mtgdecks.net/decks/csv/Standard), [Modern](https://mtgdecks.net/decks/csv/Modern), [Legacy](https://mtgdecks.net/decks/csv/Legacy), [Vintage](https://mtgdecks.net/decks/csv/Vintage), [Commander](https://mtgdecks.net/decks/csv/Commander), [Pauper](https://mtgdecks.net/decks/csv/Pauper) [Frontier](https://mtgdecks.net/decks/csv/Frontier), [Peasant](https://mtgdecks.net/decks/csv/Peasant) or [Highlander](https://mtgdecks.net/decks/csv/Highlander)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "The usual first step of machine learning tasks is getting the data to the right form for the algorithm that we wish to apply. The data is a csv file where each line represents a decklist. Each line contains a main deck and sideboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"4 Celestial Colonnade 1 Celestial Purge 3 Cryptic Command 2 Detention Sphere 1 Disdainful Stroke 1 Dispel 1 Elspeth, Sun's Champion 2 Field of Ruin 4 Flooded Strand 1 Geist of Saint Traft 2 Ghost Quarter 1 Gideon Jura 2 Gideon of the Trials 1 Gideon, Ally of Zendikar 3 Glacial Fortress 1 Grafdigger's Cage 1 Hallowed Fountain 5 Island 1 Jace, Architect of Thought 3 Leyline of Sanctity 3 Mana Leak 2 Negate 4 Path to Exile 3 Plains 2 Rest in Peace 1 Search for Azcanta 4 Serum Visions 3 Snapcaster Mage 1 Sphinx's Revelation 4 Spreading Seas 2 Stony Silence 3 Supreme Verdict 1 Temple of Enlightenment 1 Think Twice 1 Vendilion Clique\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('Modern.htm', 'r') as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the data into a gensim Dictionary, similarly as in [this tutorial](https://radimrehurek.com/gensim/tut1.html). We split each decklist into individual cards, ignoring the card counts and cards that appear only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re \n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([x.strip() for x in re.split(r\"[\\d]+\", line.replace(\"\\\"\", \"\"))]\n",
    "                                for line in open('Modern.htm'))\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)  # remove cards that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 700 cards in our modern-card dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n"
     ]
    }
   ],
   "source": [
    "unique_cards = len(dictionary.keys())\n",
    "print(unique_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a gensim Corpus. Instead of having a bag of words (cards) model, we take note how many times each card appears in a deck and \"uncompress\" the decklist description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('Modern.htm'):\n",
    "            decklist = line.replace(\"\\\"\", \"\") # remove start and end tokens            \n",
    "            decklist = re.split(r\"([\\d]+)\", decklist) # split by numbers and card names\n",
    "            decklist = [x.strip() for x in decklist] # remove whitespace\n",
    "            decklist = filter(None, decklist) # remove empty words\n",
    "            cleaned_decklist = [] \n",
    "            for i in range(len(decklist)/2): # remove numbers from decklist, add multiplicities of cards\n",
    "                for j in range(int(decklist[i*2])):\n",
    "                    cleaned_decklist.append(decklist[i*2+1])\n",
    "            yield dictionary.doc2bow(cleaned_decklist)\n",
    "corpus_memory_friendly = MyCorpus()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now that the data is ready, we set the amount of achetypes to be found. Setting it to 30 gave me good results. Try varying this and see what happens! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archetypes = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are stochastic steps in the training of the model, you might get slightly different results each time. Having the seed set to 1 allows you to recreate my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Latent Dirichlet\" part of the method name comes from the assumption that latent [priors](https://en.wikipedia.org/wiki/Prior_probability) on the per-archetype card distribution and per-decklist archetype distributions are [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution). This allows us to steer the learning of the model.\n",
    "\n",
    "The priors allow us to tell the model how we believe the data actually looks like. If we have a large number of archetypes and are confident that each decklist only falls under one archetype, than setting a low alpha indicates that we prefer each decklist to belong to few, dominating archetypes. We can similarly control the archetype-card sparsity with beta. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_prior = [1.0 / archetypes] * archetypes\n",
    "beta_prior = [1.0 / archetypes] * unique_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally train the model. This could take a couple of minutes. It seems like a good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterations = 30\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus_memory_friendly, id2word=dictionary, num_topics=archetypes,\n",
    "                                      passes=iterations, alpha = alpha_prior, eta = beta_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "A good rule of thumb while doing machine learning work is to do regular sanity checks. Anything from simple output prints to beautiful visualizations will help you understand what's going on. After the training is finished, we can explore the archetypes that it finds. Gensim offers a nice way to see the probability-card pairs in each archetype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archetype 0 \n",
      " 0.149*\"Plains\" + 0.064*\"Path to Exile\" + 0.049*\"Thalia, Guardian of Thraben\" + 0.048*\"Ghost Quarter\" + 0.047*\"Restoration Angel\" + 0.046*\"Aether Vial\" + 0.044*\"Flickerwisp\" + 0.041*\"Blade Splicer\" + 0.039*\"Thraben Inspector\" + 0.036*\"Stony Silence\" + 0.034*\"Tectonic Edge\" + 0.033*\"Leonin Arbiter\" + 0.032*\"Rest in Peace\" + 0.028*\"Horizon Canopy\" + 0.022*\"Leyline of Sanctity\" + 0.020*\"Mirran Crusader\" \n",
      "\n",
      "Archetype 1 \n",
      " 0.078*\"Mountain\" + 0.060*\"Primeval Titan\" + 0.046*\"Wooded Foothills\" + 0.045*\"Search for Tomorrow\" + 0.045*\"Sakura-Tribe Elder\" + 0.044*\"Valakut, the Molten Pinnacle\" + 0.041*\"Stomping Ground\" + 0.039*\"Scapeshift\" + 0.036*\"Summoner's Pact\" + 0.035*\"Farseek\" + 0.034*\"Forest\" + 0.034*\"Relic of Progenitus\" + 0.033*\"Lightning Bolt\" + 0.032*\"Cinder Glade\" + 0.029*\"Anger of the Gods\" + 0.028*\"Obstinate Baloth\" \n",
      "\n",
      "Archetype 2 \n",
      " 0.055*\"Knight of the Reliquary\" + 0.054*\"Noble Hierarch\" + 0.054*\"Windswept Heath\" + 0.052*\"Collected Company\" + 0.048*\"Path to Exile\" + 0.045*\"Birds of Paradise\" + 0.040*\"Forest\" + 0.035*\"Voice of Resurgence\" + 0.031*\"Ghost Quarter\" + 0.029*\"Temple Garden\" + 0.025*\"Spell Queller\" + 0.024*\"Courser of Kruphix\" + 0.023*\"Plains\" + 0.022*\"Scavenging Ooze\" + 0.021*\"Selfless Spirit\" + 0.020*\"Wooded Foothills\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_top_cards = 16\n",
    "archetypes_to_inspect = 3 \n",
    "for i in range(archetypes_to_inspect):\n",
    "    print((\"Archetype %i \\n %s \\n\") % (i, lda.print_topic(i, topn=number_of_top_cards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is generative, we can generate new decks as well. Here's an example of how to make an affinity deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affinity for AI\n",
      "4 Blinkmoth Nexus\n",
      "4 Darksteel Citadel\n",
      "4 Mox Opal\n",
      "4 Signal Pest\n",
      "4 Inkmoth Nexus\n",
      "4 Arcbound Ravager\n",
      "4 Cranial Plating\n",
      "4 Ornithopter\n",
      "4 Springleaf Drum\n",
      "4 Vault Skirge\n",
      "4 Steel Overseer\n",
      "4 Etched Champion\n",
      "3 Galvanic Blast\n",
      "2 Master of Etherium\n",
      "3 Spire of Industry\n",
      "1 Memnite\n",
      "3 Ghirapur Aether Grid\n",
      "2 Ancient Grudge\n",
      "3 Glimmervoid\n",
      "2 Grafdigger's Cage\n",
      "2 Spell Pierce\n",
      "1 Whipflare\n",
      "1 Rest in Peace\n",
      "2 Relic of Progenitus\n",
      "1 Vandalblast\n",
      "1 Gemstone Caverns\n"
     ]
    }
   ],
   "source": [
    "archetype_id = 13\n",
    "archetype_topic = np.array(lda.show_topic(archetype_id, topn=9999))\n",
    "\n",
    "archetype_distribution = np.array(archetype_topic[:,1], dtype=\"float32\")\n",
    "archetype_distribution = archetype_distribution / np.sum(archetype_distribution)\n",
    "\n",
    "archetype_indices = np.zeros(len(archetype_distribution))\n",
    "main_deck = 60\n",
    "sideboard = 15\n",
    "while np.sum(archetype_indices) < main_deck+sideboard:\n",
    "    new_card = np.random.multinomial(1, archetype_distribution)\n",
    "    archetype_indices += new_card\n",
    "    if 5 in archetype_indices:\n",
    "        archetype_indices -= new_card\n",
    "archetype_cards = np.array(archetype_topic[:,0], dtype=\"string\")\n",
    "minimum_cards = 1.0\n",
    "deck_title = \"Affinity for AI\"\n",
    "print(deck_title)\n",
    "for i in range(len(archetype_distribution)):\n",
    "    if archetype_indices[i] >= minimum_cards:        \n",
    "        print('%i %s' % (archetype_indices[i], archetype_cards[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
